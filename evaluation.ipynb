{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/5_kings_all.json', 'r') as f:\n",
    "    qa_dataset = json.loads(f.read())\n",
    "\n",
    "with open('data/knowledge_graph_query_result.json', 'r') as f:\n",
    "    knowledge_graph_query_result_set = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human_eval_set = []\n",
    "\n",
    "# id = 0\n",
    "# for qa, knowledge_graph_query_result, model_result in zip(qa_dataset, knowledge_graph_query_result_set, gpt4_result_set):\n",
    "#     human_eval_set.append({\n",
    "#         'id': id,\n",
    "#         'context': qa['context'],\n",
    "#         'question': qa['question'],\n",
    "#         'answer': qa['answer'],\n",
    "#         'model_result': model_result,\n",
    "#     })\n",
    "\n",
    "#     id += 1\n",
    "\n",
    "# human_eval_set\n",
    "# with open('data/human_set.json', 'w') as f:\n",
    "# f.write(json.dumps(human_eval_set, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model_result_set):\n",
    "    metric = {\n",
    "        'correct': {\n",
    "            'in_knowledge_graph': 0,\n",
    "            'not_in_knowledge_graph': 0,\n",
    "        },\n",
    "        'incorrect': {\n",
    "            'in_knowledge_graph': 0,\n",
    "            'not_in_knowledge_graph': 0,\n",
    "        },\n",
    "    }\n",
    "    for qa, knowledge_graph_query_result, model_result in zip(qa_dataset, knowledge_graph_query_result_set, model_result_set):\n",
    "        mask = metric\n",
    "\n",
    "        found_in_answer = False\n",
    "        answer = qa['answer']\n",
    "        if any(token in answer for token in model_result.replace('.', ' ').strip().split()):\n",
    "            found_in_answer = True\n",
    "        mask = mask['correct'] if found_in_answer else mask['incorrect']\n",
    "\n",
    "        found_in_knoledge_graph = False\n",
    "        for entity in knowledge_graph_query_result:\n",
    "            if any(token in entity for token in model_result.replace('.', ' ').strip().split()):\n",
    "                found_in_knoledge_graph = True\n",
    "                break\n",
    "        if found_in_knoledge_graph:\n",
    "            mask['in_knowledge_graph'] += 1\n",
    "        else:\n",
    "            mask['not_in_knowledge_graph'] += 1\n",
    "\n",
    "    model_accuracy = (metric['correct']['in_knowledge_graph'] + metric['correct']['not_in_knowledge_graph']) / \\\n",
    "        (metric['correct']['in_knowledge_graph'] +\n",
    "         metric['correct']['not_in_knowledge_graph'] +\n",
    "         metric['incorrect']['in_knowledge_graph'] +\n",
    "         metric['incorrect']['not_in_knowledge_graph'])\n",
    "\n",
    "    print('# Model Result')\n",
    "    print(f'Accurracy: {model_accuracy}')\n",
    "\n",
    "    print('====================')\n",
    "\n",
    "    tp = metric['correct']['in_knowledge_graph']\n",
    "    fp = metric['incorrect']['in_knowledge_graph']\n",
    "    fn = metric['correct']['not_in_knowledge_graph']\n",
    "    tn = metric['incorrect']['not_in_knowledge_graph']\n",
    "\n",
    "    print('# Knowledge Graph Result')\n",
    "    print(f'TP: {tp}, FP: {fp}')\n",
    "    print(f'FN: {fn}, TN: {tn}')\n",
    "    print(f'Accuracy: {(tp + tn) / (tp + tn + fp + fn)}')\n",
    "    if tp + fp == 0:\n",
    "        print(f'Precision: NaN')\n",
    "    else:\n",
    "        print(f'Precision: {tp / (tp + fp)}')\n",
    "    print(f'Recall: {tp / (tp + fn)}')\n",
    "    print(f'F1: {2 * tp / (2 * tp + fp + fn)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model Result\n",
      "Accurracy: 0.26693227091633465\n",
      "====================\n",
      "# Knowledge Graph Result\n",
      "TP: 31, FP: 14\n",
      "FN: 36, TN: 170\n",
      "Accuracy: 0.8007968127490039\n",
      "Precision: 0.6888888888888889\n",
      "Recall: 0.4626865671641791\n",
      "F1: 0.5535714285714286\n"
     ]
    }
   ],
   "source": [
    "with open('data/llama2_result.txt', 'r') as f:\n",
    "    result = f.read().split('\\n')\n",
    "    eval(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model Result\n",
      "Accurracy: 0.2589641434262948\n",
      "====================\n",
      "# Knowledge Graph Result\n",
      "TP: 34, FP: 11\n",
      "FN: 31, TN: 175\n",
      "Accuracy: 0.8326693227091634\n",
      "Precision: 0.7555555555555555\n",
      "Recall: 0.5230769230769231\n",
      "F1: 0.6181818181818182\n"
     ]
    }
   ],
   "source": [
    "with open('data/llama2_fine_tuned_result.txt', 'r') as f:\n",
    "    result = f.read().split('\\n')\n",
    "    eval(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model Result\n",
      "Accurracy: 0.2788844621513944\n",
      "====================\n",
      "# Knowledge Graph Result\n",
      "TP: 34, FP: 15\n",
      "FN: 36, TN: 166\n",
      "Accuracy: 0.796812749003984\n",
      "Precision: 0.6938775510204082\n",
      "Recall: 0.4857142857142857\n",
      "F1: 0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "with open('data/gpt3_5_result.txt', 'r') as f:\n",
    "    result = f.read().split('\\n')\n",
    "    eval(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model Result\n",
      "Accurracy: 0.33067729083665337\n",
      "====================\n",
      "# Knowledge Graph Result\n",
      "TP: 36, FP: 9\n",
      "FN: 47, TN: 159\n",
      "Accuracy: 0.7768924302788844\n",
      "Precision: 0.8\n",
      "Recall: 0.43373493975903615\n",
      "F1: 0.5625\n"
     ]
    }
   ],
   "source": [
    "with open('data/gpt3_5_fine_tuned_result.txt', 'r') as f:\n",
    "    result = f.read().split('\\n')\n",
    "    eval(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model Result\n",
      "Accurracy: 0.3466135458167331\n",
      "====================\n",
      "# Knowledge Graph Result\n",
      "TP: 43, FP: 9\n",
      "FN: 44, TN: 155\n",
      "Accuracy: 0.7888446215139442\n",
      "Precision: 0.8269230769230769\n",
      "Recall: 0.4942528735632184\n",
      "F1: 0.6187050359712231\n"
     ]
    }
   ],
   "source": [
    "with open('data/gpt4_result.txt', 'r') as f:\n",
    "    result = f.read().split('\\n')\n",
    "    eval(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
